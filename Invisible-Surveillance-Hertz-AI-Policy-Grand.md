# Invisible Surveillance, Visible Harm: What the Hertz Case Teaches Us About Unregulated AI

**By David Grand**

In the race to modernize, Hertz Rent-A-Car has embraced artificial intelligence (AI) and automation across its operations—from billing and fleet tracking to fraud detection and customer management. But as the company has learned, AI-driven systems without transparency or accountability can create real harm for real people. Hundreds of customers have been wrongfully charged, banned without explanation, or even reported to law enforcement—all because of flawed or unchecked automated systems.

What happened at Hertz is not simply a business systems failure. It is a warning shot. We are entering a new phase of **invisible surveillance**—where individuals are judged, charged, or criminalized by algorithms embedded in everyday technologies, and where the line between automation and oversight has all but vanished.

---

## AI Misjudgment in a Black Box

In multiple cases, Hertz’s automated systems have issued bills for cars that had already been returned, charged customers for rentals that never occurred, or permanently banned drivers based on opaque internal data. Some of these bans and charges were made with no recourse for appeal and no contact from a human representative. In some cases, customers were falsely reported for theft—a deeply serious accusation that led to wrongful arrests, all based on outdated or improperly synced data.

This is not a glitch. It’s a system of **automated decision-making without human oversight**. These incidents have exposed the brittleness of AI systems when applied to high-stakes decisions like criminal allegations, credit-based denial of service, or banning individuals from access to essential goods like transportation.

---

## Third-Party AI, No Public Accountability

Much of this behavior stems from Hertz’s integration with third-party platforms for fraud detection, fleet telemetry, and billing. These companies operate behind the scenes—collecting, analyzing, and making decisions based on data that customers are never shown.

But third-party involvement also muddies the lines of accountability. When a customer is banned or charged in error, who do they contact? The rental desk? The billing system provider? The AI vendor? The result is a Kafkaesque loop: a closed system in which individuals are surveilled and judged by algorithms, but can’t challenge or even understand the decision-making process.

---

## The Future of Surveillance Is Subtle—and Dangerous

Hertz represents something bigger than a broken billing system. It’s a microcosm of the **invisible surveillance economy**. Just as wearable AI devices like smart glasses quietly collect and process visual data without overt signals of surveillance, these corporate AI systems monitor user behavior, assess risk, and impose consequences—with **no notification, no consent, and no transparency**.

What makes this so dangerous is precisely what makes it feel “safe”: the surveillance does not feel like surveillance. It is not a camera in the corner or a badge-wearing officer. It’s lines of code in a system that quietly flags you, blocks you, or bills you without your knowledge.

This is not a dystopian future—it is the present. And without meaningful policy reform, it will become standard business practice.

---

## Policy Recommendations

To avoid further harm and create ethical guardrails around the use of AI in consumer services, policymakers and regulators should pursue the following:

### 1. Mandate Algorithmic Transparency  
Companies using automated decision-making tools—especially those that impact access to goods, credit, or legal status—must disclose when and how those systems are used. Consumers should be informed when an algorithm is used to make a significant determination about them.

### 2. Ensure Human-in-the-Loop Standards  
No customer should be banned, charged, or criminalized based solely on an AI decision. Human review must be required before serious outcomes are enacted.

### 3. Create a Consumer AI Bill of Rights  
Consumers deserve clear protections when interacting with automated systems. This includes the right to know, the right to opt out, the right to challenge decisions, and the right to correction.

### 4. Hold Companies Accountable for Third-Party Errors  
Firms like Hertz must be held responsible for the decisions made by third-party systems they integrate. “We didn’t know the vendor did that” is not an acceptable legal or ethical defense.

### 5. Strengthen Data Governance Standards  
Companies should be required to demonstrate that their data is accurate, synchronized, and regularly audited—especially if it is being used to drive automated decisions that affect people’s freedom, reputation, or finances.

---

## Conclusion: Technology Must Serve People, Not the Other Way Around

The integration of AI into everyday systems like rental cars, public infrastructure, or consumer services is not inherently bad. But without ethical standards and legal safeguards, we risk creating a society where people are judged, denied, or punished without ever facing a human being.

Companies like Hertz are not just transportation providers anymore. They are data collectors, algorithmic arbiters, and—increasingly—surveillance actors. As we move further into a world where AI quietly governs the systems we rely on, we must act now to ensure those systems remain accountable, transparent, and fair.

Otherwise, we are building a future in which **surveillance doesn’t feel like surveillance—but harms like it always has**.

---

**David Grand** is a global business leader and researcher exploring the intersection of AI, policy, and ethics. He is currently studying international business and AI policy at Thunderbird Global Management School.
